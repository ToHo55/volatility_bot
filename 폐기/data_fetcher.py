import os
import time
import logging
import pyupbit
import pandas as pd
from datetime import datetime, timedelta
import pickle
import glob
from pathlib import Path
import hashlib
from typing import Optional

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
CACHE_DIR = BASE_DIR / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
CACHE_MAX_AGE_DAYS = 7

# ë¡œê·¸ ì´ëª¨ì§€ ì •ì˜
class LogEmoji:
    INFO = "â„¹ï¸"
    WARNING = "âš ï¸"
    ERROR = "âŒ"
    SUCCESS = "âœ…"
    LOADING = "â³"
    DATA = "ğŸ“Š"
    CACHE = "ğŸ’¾"
    TRADE = "ğŸ’°"
    TIME = "â°"
    PROFIT = "ğŸ’¹"
    LOSS = "ğŸ“‰"
    ENTRY = "ğŸ“ˆ"
    EXIT = "ğŸ“‰"
    RISK = "âš ï¸"
    POSITION = "ğŸ“Š"
    MARKET = "ğŸ›ï¸"
    SIGNAL = "ğŸ””"
    SUMMARY = "ğŸ“Š"

def print_progress(current: int, total: int, prefix: str = '', suffix: str = '', decimals: int = 1, length: int = 50, fill: str = 'â–ˆ', print_end: str = "\r"):
    percent = ("{0:." + str(decimals) + "f}").format(100 * (current / float(total)))
    filled_length = int(length * current // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)
    if current == total:
        print()

def safe_request_with_retry(func, *args, max_retries: int = 5, delay: float = 1.0, **kwargs):
    for attempt in range(max_retries):
        try:
            result = func(*args, **kwargs)
            if result is not None:
                return result
        except Exception as e:
            logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
        time.sleep(delay * (2 ** attempt))
    return None

def get_cache_key(ticker: str, start_date: str, end_date: str) -> str:
    """ìºì‹œ í‚¤ ìƒì„± í•¨ìˆ˜"""
    try:
        # ë‚ ì§œë¥¼ ë…„ì›” í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì¼ë‹¨ìœ„ê°€ ì•„ë‹Œ ì›”ë‹¨ìœ„ë¡œ ê´€ë¦¬)
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y%m")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y%m")
        
        # ìºì‹œ í‚¤ í˜•ì‹: ticker_YYYYMM_YYYYMM_interval
        key = f"{ticker}_{start_dt}_{end_dt}_minute5"
        
        # í•´ì‹œ ìƒì„± (16ì)
        cache_key = hashlib.md5(key.encode()).hexdigest()[:16]
        logger.debug(f"ìºì‹œ í‚¤ ìƒì„±: {key} -> {cache_key}")
        return cache_key
    except Exception as e:
        logger.error(f"ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return hashlib.md5(f"{ticker}_{start_date}_{end_date}".encode()).hexdigest()[:16]

def find_matching_cache(ticker: str, start_date: str, end_date: str):
    """ë¹„ìŠ·í•œ ê¸°ê°„ì˜ ìºì‹œëœ ë°ì´í„° ì°¾ê¸°"""
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # ì‹œê°„ ì •ë³´ ì œê±°
        start_dt = start_dt.replace(hour=0, minute=0, second=0, microsecond=0)
        end_dt = end_dt.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # í—ˆìš© ì˜¤ì°¨ ë²”ìœ„ í™•ëŒ€ (7ì¼ -> 30ì¼)
        allowed_diff = timedelta(days=30)
        cache_files = glob.glob(os.path.join(CACHE_DIR, "*.pkl"))
        
        best_match = None
        best_coverage = 0
        
        for cache_file in cache_files:
            try:
                with open(cache_file, "rb") as f:
                    data = pickle.load(f)
                    if not isinstance(data, pd.DataFrame) or data.empty:
                        continue
                    
                    # ë°ì´í„°ì˜ ì‹œì‘/ì¢…ë£Œì¼ í™•ì¸
                    data_start = data.index[0].replace(hour=0, minute=0, second=0, microsecond=0)
                    data_end = data.index[-1].replace(hour=0, minute=0, second=0, microsecond=0)
                    
                    # ë°ì´í„° ê¸°ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ (ìµœì†Œ 90ì¼)
                    if (data_end - data_start).days < 90:
                        continue
                    
                    # ìš”ì²­ ê¸°ê°„ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                    if data_start <= start_dt + allowed_diff and data_end >= end_dt - allowed_diff:
                        coverage = (min(data_end, end_dt) - max(data_start, start_dt)).days
                        
                        if coverage > best_coverage:
                            best_coverage = coverage
                            best_match = data
                            logger.info(f"ë” ë‚˜ì€ ìºì‹œ ë§¤ì¹˜ ë°œê²¬: {os.path.basename(cache_file)}")
                            logger.info(f"  - ì»¤ë²„ë¦¬ì§€: {coverage}ì¼")
                            logger.info(f"  - ê¸°ê°„: {data_start.date()} ~ {data_end.date()}")
                
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        if best_match is not None:
            # ë°ì´í„° í•„í„°ë§ ì‹œ ì—¬ìœ  ê¸°ê°„ í¬í•¨
            mask = (best_match.index >= start_dt - allowed_diff) & (best_match.index <= end_dt + allowed_diff)
            filtered_data = best_match[mask].copy()
            
            if not filtered_data.empty and len(filtered_data) >= 1000:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
                return filtered_data
        
        return None
        
    except Exception as e:
        logger.error(f"ìºì‹œ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_cached_data(cache_key: str):
    try:
        cache_file = os.path.join(CACHE_DIR, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, "rb") as f:
                data = pickle.load(f)
                if isinstance(data, pd.DataFrame) and not data.empty:
                    return data
        return None
    except Exception as e:
        logger.error(f"ìºì‹œ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def clean_old_cache(cache_dir: Path = CACHE_DIR, max_age_days: int = CACHE_MAX_AGE_DAYS):
    """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬"""
    try:
        now = datetime.now().timestamp()
        for cache_file in cache_dir.glob("*.pkl"):
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    if (now - data['timestamp']) > max_age_days * 86400:
                        cache_file.unlink()
            except:
                cache_file.unlink()  # ì†ìƒëœ ìºì‹œ íŒŒì¼ ì‚­ì œ
                
    except Exception as e:
        logger.error(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def save_to_cache(df: pd.DataFrame, cache_key: str) -> bool:
    """ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
    try:
        if df.empty or len(df) < 1000:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
            logger.warning("ìºì‹œ ì €ì¥ ì‹¤íŒ¨: ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤")
            return False
            
        cache_file = os.path.join(CACHE_DIR, f"{cache_key}.pkl")
        
        # ê¸°ì¡´ ìºì‹œ íŒŒì¼ ë°±ì—…
        if os.path.exists(cache_file):
            backup_file = cache_file + ".bak"
            try:
                os.rename(cache_file, backup_file)
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
        
        # ìƒˆ ë°ì´í„° ì €ì¥
        with open(cache_file, "wb") as f:
            pickle.dump(df, f)
            
        logger.info(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(df):,}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
        
        # ë°±ì—… íŒŒì¼ ì‚­ì œ
        if os.path.exists(backup_file):
            try:
                os.remove(backup_file)
            except Exception as e:
                logger.warning(f"ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        
        return True
        
    except Exception as e:
        logger.error(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def fetch_ohlcv(ticker: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
    """OHLCV ë°ì´í„° ì¡°íšŒ (ìºì‹œ ì‹œìŠ¤í…œ í¬í•¨)"""
    try:
        cache_key = get_cache_key(ticker, start_date, end_date)
        cache_file = CACHE_DIR / f"{cache_key}.pkl"
        
        # ìºì‹œ í™•ì¸
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
                if (datetime.now().timestamp() - data['timestamp']) < CACHE_MAX_AGE_DAYS * 86400:
                    return data['data']
        
        # ìƒˆë¡œìš´ ë°ì´í„° ì¡°íšŒ
        df = pyupbit.get_ohlcv(ticker, interval="minute5", to=end_date, count=1000)
        if df is not None and len(df) > 0:
            # ìºì‹œ ì €ì¥
            cache_data = {
                'ticker': ticker,
                'data': df,
                'timestamp': datetime.now().timestamp()
            }
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            return df
            
        return None
        
    except Exception as e:
        logging.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ({ticker}): {str(e)}")
        return None 